---
title: "Week 8 seminar: nonlinearity and nearest neighbors"
author:
  - Prof. Joshua Loftus (lecturer)
  - Shakeel GAvioli-Akilagun (GTA)
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
library(modelr)
```

**Important** We will be doing some simulations, set a random seed so your work is reproducible. 

```{r}
set.seed(42)
```

## 2d non-linear classification data

Assign functions with zero set that defines the perfect (Bayes) decision boundary:

```{r}
nn <- 800

true_boundary_function <- function(x1, x2) {
  thing <- x1^2*x2^3
  (x1^2 + x2^2 - 1)^3 - thing^2
}
```

Generate some training data where your training data should take the form of a data frame with columns: 

 * `x1` and `x2` for the predictors
 * `seperable` for class labels if the classes were perfectly separable
 * `yy` for class labels with some noise

```{r}
train <- data.frame(
  x1 = 1.5*(1 - 2*runif(nn)),
  x2 = 1.5*(1 - 2*runif(nn))
) %>% 
  mutate(
    separable = true_boundary_function(x1,x2) > 0,
    yy = factor(rbinom(nn, 1, 9/10 - (8/10) * as.numeric(separable)))
  )
```

### Plot the data

```{r}
train_plot <-
  ggplot(train, aes(x1, x2)) +
  geom_point(aes(color = yy)) +
  xlab("") + ylab("")
  
train_plot
```

### Plot the Bayes decision boundary

```{r}
decision_surface <- 
  data_grid(train,
          x1 = seq_range(x1, 300, expand = .05),
          x2 = seq_range(x2, 300, expand = .05)) %>%
  mutate(z = true_boundary_function(x1, x2))

boundary_plot <-
  train_plot +
  geom_contour(
    data = decision_surface,
    aes(x1, x2, z=z),
    bins = 2,
    size = 1,
    color = "black",
    alpha = .5)

boundary_plot
```

### How well does linear classification do?

Also try changing the formula to fit a logistic regression model with non-linear transformations of the predictors

```{r}
logit_model <-  
  glm(yy ~ x1 + x2 + poly(x1,2) * poly(x2,2), family = "binomial", data = train)

logit_surface <- logit_model %>%
   augment(type.predict = "response",
              newdata = decision_surface)
```

Plot decision boundary of logistic model

```{r}
boundary_plot +
  geom_contour(
    data = logit_surface,
    aes(x1, x2, z = .fitted),
    size = 1,
    color = "green") 
```

### Classify 2d example with k-nearest neighbors

Write a function to implement k-NN classification

You may use matrix operations and simple functions like `sort` but not any more sophisticated built-in R functions (or libraries)

#### Write a function that computes some distance between two points

You can choose which notion of distance you want to use

```{r}
Dist <- function(x1, x2) sum((x1-x2)**2) 
```

#### Write a function that outputs the y values for the k-nearest nearest neighbors of a point x0

```{r}
nearest_y_values <- function(x0, kk, xx, yy) 
  {
  #'
  #'
  #'@param x0 vector, 
  #'@param kk int, number of neigbours
  #'@param xx matrix, predictors
  #'@param yy vector, classes
  
  mm <- nrow(xx)
  
  distances <- rep(0, nn)
  
  for (ii in 1:mm) distances[ii] <- Dist(x0, xx[ii,])
    
  k_neighborhood <- order(distances, decreasing = FALSE)[1:kk]
  
  return(y[k_neighborhood])
}
```

#### Write a function that outputs a classification using majority vote

```{r}
knn <- function(x0, kk, xx, yy) 
  {
  #'
  #'
  #'@param x0 vector, 
  #'@param kk int, number of neigbours
  #'@param xx matrix, predictors
  #'@param yy vector, classes
  
  votes <- nearest_y_values(x0, kk, xx, yy)
  
  levels(y)[which.max(table(votes))]
  
  }
```


#### Evaluate your function at various points, using various values of k, and compare the output to the plotted training data

Try points where the nearest neighbors are clearly majority 0 or 1, and a point where the portion is relatively even. Run the code below a few times and see if the answers change, then pick a different `K` and try again

```{r}
bootstrap_sample <- sample(1:nrow(train), replace = TRUE)
x <- train[bootstrap_sample, 1:2]
y <- train$y[bootstrap_sample]
```


In a majority 1 region

```{r}
knn(x0 = c(0,0), kk = 1, xx = x, yy = y)
```

In a majority 0 region

```{r}
knn(x0 = c(0,1.2), kk = 1, xx = x, yy = y)
```

In a relatively even region

```{r}
knn(x0 = c(1,1), kk = 1, xx = x, yy = y)
```
